# ğŸ§ª Vald8 â€” Lightweight Evaluation Framework for LLM Reliability

Vald8 is a minimalist, developer-first SDK for testing LLM-powered Python functions using structured JSONL datasets.

It provides a simple way to validate:

- **Schema correctness**
- **Instruction adherence**
- **Reference accuracy**
- **Keyword / regex expectations**

With optional support for **LLM-as-Judge** scoring.

Focus: **Make LLM evaluation as easy as pytest. Nothing more. Nothing less.**

---

# ğŸš€ Why Vald8?

If you're building with LLMs, you need a way to verify that your AI functions:

- produce valid JSON  
- follow instructions consistently  
- don't regress when prompts or models change  
- behave consistently across environments  
- meet quality thresholds before deployment  

Vald8 gives you this with:

- âœ” One decorator  
- âœ” One JSONL file  
- âœ” One evaluation call  

No configuration. No complexity. No over-engineering.

---

# ğŸ“¦ Install

```bash
pip install vald8
```

---

# ğŸ§© Core Concept

You decorate any LLM function:

```python
from vald8 import vald8

@vald8(dataset="tests.jsonl")
def generate(prompt: str) -> dict:
    ...
```

Vald8 loads your dataset, runs the function against each example, and scores the results.

---

# ğŸ“ JSONL Test Dataset Example

Save as `tests.jsonl`:

```json
{"id": "math1", "input": "What is 2+2?", "expected": {"reference": "4"}}
{"id": "json1", "input": "Return JSON with name and age", "expected": {"schema": {"type": "object", "properties": {"name": {"type": "string"}, "age": {"type": "number"}}, "required": ["name", "age"]}}}
{"id": "hello1", "input": "Greet politely", "expected": {"contains": ["hello", "please"]}}
{"id": "regex1", "input": "Give a date", "expected": {"regex": "\d{4}-\d{2}-\d{2}" }}
```

Supported expectations:

- `"reference": "exact value"`
- `"contains": ["word1", "word2"]`
- `"regex": "pattern"`
- `"schema": {...}`  

---

# ğŸ§ª Decorating an LLM Function

```python
from vald8 import vald8
import openai

@vald8(dataset="tests.jsonl")
def generate(prompt: str) -> dict:
    response = openai.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"}
    )
    return {"response": response.choices[0].message.content}
```

---

# ğŸ“Š Running Evaluations

```python
results = generate.run_eval()

print("Passed:", results["passed"])
print("Success Rate:", results["summary"]["success_rate"])
print("Details saved to:", results["run_dir"])
```

Example output:

```
âœ” math1
âœ” json1
âœ– hello1 â€” missing: please
âœ” regex1

Overall: 3/4 passed (75%)
```

---

# ğŸ§± Optional: LLM-as-Judge Scoring

Useful for long-form or fuzzy outputs.

```python
@vald8(
    dataset="tests.jsonl",
    judge_provider="openai"   # or "anthropic", "local"
)
def summarize(text: str) -> str:
    return llm_summarize(text)
```

Most tests require **no API calls**.

---

# ğŸ§© CI/CD Integration

```yaml
- name: Run Vald8 Tests
  run: |
    python -c "
    from my_llm import generate
    assert generate.run_eval()['passed']
    "
```

---

# ğŸ“ Results Format

Each run produces:

```
runs/
â””â”€â”€ 2025-11-21_12-01-44/
    â”œâ”€â”€ results.jsonl
    â”œâ”€â”€ summary.json
    â””â”€â”€ metadata.json
```

---

# ğŸ”§ Configuration Options

```python
@vald8(
    dataset="tests.jsonl",
    tests=["schema", "contains", "reference"],
    thresholds={"success_rate": 0.9},
    sample_size=None,
    cache=False,
    judge_provider=None,
)
```

All parameters are optional.

---

# ğŸ›  Minimal Feature Set (v0.1)

Included:

- âœ” Test decorator  
- âœ” JSONL dataset loader  
- âœ” Schema validation  
- âœ” Contains / reference / regex checks  
- âœ” Optional LLM-as-judge  
- âœ” Clear results + artifacts  
- âœ” Offline mode  
- âœ” CI/CD-ready  
- âœ” Zero-config defaults  

---

# ğŸ¤ Contributing

PRs welcome.

---

# ğŸ“œ License

MIT License â€” free and open source.
