# üèóÔ∏è Vald8 Architecture & Implementation Specification

This document provides the complete technical specification for Vald8 MVP implementation.  
It serves as the source of truth for contributors and maintainers.

---

## üéØ Core Design Principles

### Foundational Principles
1. **Readable First** ‚Äî Anyone should understand a module in <5 minutes
2. **Small Files** ‚Äî Target <300 lines per file, split early
3. **Flat Imports** ‚Äî Public APIs in `__init__.py`, no deep nesting
4. **Early Returns** ‚Äî Avoid nested conditionals, fail fast
5. **Protocol-based** ‚Äî Use Python protocols for interfaces
6. **Functional Core** ‚Äî Pure functions where possible, state at edges
7. **DRY but Clear** ‚Äî Reuse without over-abstraction

### Maintainability Patterns
8. **Type-First Design** ‚Äî Strong typing with clear interfaces and contracts
9. **Rich Error Context** ‚Äî Detailed errors with location, message, type, and context
10. **Fail-Fast with Details** ‚Äî Collect validation errors, provide actionable feedback
11. **Explicit Serialization** ‚Äî Clean JSON/dict conversion with validation
12. **Configuration Objects** ‚Äî Structured config with validation and defaults
13. **Computed Properties** ‚Äî Lazy evaluation for derived values
14. **Layered Validation** ‚Äî Multiple validation phases with clear responsibilities
15. **Consistent API Surface** ‚Äî Predictable method names and return types
16. **Graceful Degradation** ‚Äî Continue operation when possible, fail clearly when not

---

## üì¶ Package Structure (MVP)

```
vald8/
‚îú‚îÄ‚îÄ __init__.py              # Public API exports
‚îú‚îÄ‚îÄ types.py                 # Type definitions and data structures
‚îú‚îÄ‚îÄ decorator.py             # @vald8 decorator implementation  
‚îú‚îÄ‚îÄ datasets.py              # JSONL dataset loader & validator
‚îú‚îÄ‚îÄ runners.py               # Evaluation orchestration
‚îú‚îÄ‚îÄ metrics.py               # Metric functions (accuracy, schema, etc.)
‚îú‚îÄ‚îÄ cache.py                 # Optional result caching
‚îú‚îÄ‚îÄ config.py                # Configuration management
‚îú‚îÄ‚îÄ errors.py                # Custom exceptions and error types
‚îú‚îÄ‚îÄ validation.py            # Input validation utilities
‚îú‚îÄ‚îÄ serialization.py         # JSON/dict conversion utilities
‚îú‚îÄ‚îÄ cli.py                   # Command-line interface
‚îÇ
‚îú‚îÄ‚îÄ judges/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ base.py              # JudgeClient protocol & base classes
‚îÇ   ‚îú‚îÄ‚îÄ heuristics.py        # Rule-based evaluation
‚îÇ   ‚îú‚îÄ‚îÄ openai_judge.py      # OpenAI API adapter
‚îÇ   ‚îú‚îÄ‚îÄ anthropic_judge.py   # Anthropic API adapter
‚îÇ   ‚îú‚îÄ‚îÄ bedrock_judge.py     # AWS Bedrock adapter
‚îÇ   ‚îú‚îÄ‚îÄ local_judge.py       # Ollama/vLLM adapter
‚îÇ   ‚îî‚îÄ‚îÄ registry.py          # Judge factory & registration
‚îÇ
‚îú‚îÄ‚îÄ reports/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ json_report.py       # JSON summary generator
‚îÇ   ‚îú‚îÄ‚îÄ html_report.py       # HTML report renderer
‚îÇ   ‚îî‚îÄ‚îÄ md_report.py         # Markdown report generator
‚îÇ
‚îú‚îÄ‚îÄ storage/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ files.py             # File I/O, JSONL operations
‚îÇ   ‚îî‚îÄ‚îÄ runs.py              # Run directory management
‚îÇ
‚îî‚îÄ‚îÄ tests/
    ‚îú‚îÄ‚îÄ test_types.py
    ‚îú‚îÄ‚îÄ test_decorator.py
    ‚îú‚îÄ‚îÄ test_datasets.py
    ‚îú‚îÄ‚îÄ test_metrics.py
    ‚îú‚îÄ‚îÄ test_judges.py
    ‚îú‚îÄ‚îÄ test_reports.py
    ‚îú‚îÄ‚îÄ test_validation.py
    ‚îî‚îÄ‚îÄ fixtures/
        ‚îú‚îÄ‚îÄ sample_dataset.jsonl
        ‚îî‚îÄ‚îÄ expected_outputs/
```

---

## üîß Component Specifications

### 1. Type Definitions Module (`types.py`)

Clean type definitions inspired by Pydantic's design philosophy:

```python
from typing import Dict, List, Optional, Any, Literal, NamedTuple, Protocol, Union
from datetime import datetime
from dataclasses import dataclass, field
import re

# Core data structures using dataclasses (no external dependencies)

@dataclass(frozen=True)
class DatasetExample:
    """Single example from JSONL dataset with flexible input support."""
    id: str
    input: Union[str, int, float, List[Any], Dict[str, Any]]  # Flexible input type
    expected: Optional[Dict[str, Any]] = None
    meta: Optional[Dict[str, Any]] = None
    
    def __post_init__(self):
        """Validate fields after initialization."""
        if not self.id or not isinstance(self.id, str):
            raise ValueError("id must be a non-empty string")
        
        # Input can be primitive or dict
        if self.input is None:
            raise ValueError("input cannot be None")
        
        # Validate expected and meta are dicts if present
        if self.expected is not None and not isinstance(self.expected, dict):
            raise ValueError("expected must be a dictionary or None")
        if self.meta is not None and not isinstance(self.meta, dict):
            raise ValueError("meta must be a dictionary or None")
    
    def is_kwargs_input(self) -> bool:
        """Check if input should be passed as **kwargs."""
        return isinstance(self.input, dict)
    
    def get_function_args(self) -> Union[Tuple[Any], Dict[str, Any]]:
        """Get arguments to pass to function based on input type."""
        if self.is_kwargs_input():
            return self.input  # Return dict for **kwargs
        else:
            return (self.input,)  # Return tuple for positional args

@dataclass
class MetricResult:
    """Result from a single metric evaluation."""
    name: str
    score: Optional[float] = None
    passed: Optional[bool] = None
    details: str = ""
    error: Optional[str] = None
    
    def __post_init__(self):
        """Validate metric result fields."""
        if not self.name:
            raise ValueError("name is required")
        if self.score is not None:
            if not (0.0 <= self.score <= 1.0):
                raise ValueError("score must be between 0.0 and 1.0")
    
    @property
    def status(self) -> Literal['passed', 'failed', 'error', 'skipped']:
        """Computed status based on result."""
        if self.error:
            return 'error'
        if self.passed is None:
            return 'skipped'
        return 'passed' if self.passed else 'failed'
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            'name': self.name,
            'score': self.score,
            'passed': self.passed,
            'details': self.details,
            'error': self.error,
            'status': self.status
        }

@dataclass
class EvaluationResult:
    """Result from evaluating a single example."""
    id: str
    input: Dict[str, Any]
    output: Optional[Any] = None
    expected: Optional[Dict[str, Any]] = None
    metrics: Dict[str, MetricResult] = field(default_factory=dict)
    latency_ms: float = 0.0
    timestamp: Optional[datetime] = None
    error: Optional[str] = None
    
    def __post_init__(self):
        """Set defaults and validate."""
        if self.timestamp is None:
            self.timestamp = datetime.utcnow()
        if self.latency_ms < 0:
            raise ValueError("latency_ms cannot be negative")
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return {
            'id': self.id,
            'input': self.input,
            'output': self.output,
            'expected': self.expected,
            'metrics': {k: v.to_dict() for k, v in self.metrics.items()},
            'latency_ms': self.latency_ms,
            'timestamp': self.timestamp.isoformat() if self.timestamp else None,
            'error': self.error
        }

@dataclass
class MetricSummary:
    """Aggregate statistics for a metric."""
    mean: float
    min: float
    max: float
    threshold: Optional[float] = None
    passed: bool = False
    
    def __post_init__(self):
        """Validate summary statistics."""
        for val in [self.mean, self.min, self.max]:
            if not (0.0 <= val <= 1.0):
                raise ValueError(f"Metric values must be between 0.0 and 1.0, got {val}")
        if self.min > self.max:
            raise ValueError("min cannot be greater than max")
        if not (self.min <= self.mean <= self.max):
            raise ValueError("mean must be between min and max")

@dataclass
class EvaluationSummary:
    """Summary of entire evaluation run."""
    run_id: str
    total_examples: int
    passed_examples: int
    failed_examples: int
    metrics: Dict[str, MetricSummary]
    duration_seconds: float
    errors: int = 0
    
    def __post_init__(self):
        """Validate summary totals."""
        if self.total_examples < 0:
            raise ValueError("total_examples cannot be negative")
        if self.passed_examples + self.failed_examples != self.total_examples:
            raise ValueError("passed + failed must equal total examples")
        if not re.match(r'^\d{4}-\d{2}-\d{2}_\d{2}-\d{2}-\d{2}_[a-f0-9]{6}$', self.run_id):
            raise ValueError("run_id must match format YYYY-MM-DD_HH-MM-SS_xxxxxx")
    
    @property
    def success_rate(self) -> float:
        """Calculate overall success rate."""
        if self.total_examples == 0:
            return 0.0
        return self.passed_examples / self.total_examples
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            'run_id': self.run_id,
            'total_examples': self.total_examples,
            'passed_examples': self.passed_examples,
            'failed_examples': self.failed_examples,
            'success_rate': self.success_rate,
            'metrics': {k: vars(v) for k, v in self.metrics.items()},
            'duration_seconds': self.duration_seconds,
            'errors': self.errors
        }

# Configuration dataclass
@dataclass
class Vald8Config:
    """Global configuration for Vald8."""
    cache_enabled: bool = False
    cache_dir: str = ".vald8_cache"
    run_dir: str = "./runs"
    default_judge: Optional[str] = None
    api_timeout: int = 30
    max_retries: int = 3
    validate_schemas: bool = True
    
    def __post_init__(self):
        """Validate configuration values."""
        if self.api_timeout <= 0:
            raise ValueError("api_timeout must be positive")
        if self.max_retries < 0:
            raise ValueError("max_retries cannot be negative")
        if self.default_judge and self.default_judge not in ["openai", "anthropic", "bedrock", "local"]:
            raise ValueError("default_judge must be one of: openai, anthropic, bedrock, local")
    
    @classmethod
    def from_env(cls) -> 'Vald8Config':
        """Load configuration from environment variables."""
        import os
        return cls(
            cache_enabled=os.getenv('VALD8_CACHE_ENABLED', 'false').lower() == 'true',
            cache_dir=os.getenv('VALD8_CACHE_DIR', '.vald8_cache'),
            run_dir=os.getenv('VALD8_RUN_DIR', './runs'),
            default_judge=os.getenv('VALD8_DEFAULT_JUDGE'),
            api_timeout=int(os.getenv('VALD8_API_TIMEOUT', '30')),
            max_retries=int(os.getenv('VALD8_MAX_RETRIES', '3')),
            validate_schemas=os.getenv('VALD8_VALIDATE_SCHEMAS', 'true').lower() == 'true'
        )

# Judge protocol and types
class JudgeClient(Protocol):
    """Protocol for judge implementations."""
    
    def grade_instruction(
        self,
        question: str,
        answer: str,
        rubric: str,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Grade instruction adherence."""
        ...
    
    def grade_safety(
        self,
        question: str,
        answer: str,
        rubric: str,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Grade safety of response."""
        ...

@dataclass
class JudgeResponse:
    """Response from a judge evaluation."""
    score: float
    passed: bool
    reasoning: str
    confidence: Optional[float] = None
    
    def __post_init__(self):
        """Validate judge response."""
        if not (0.0 <= self.score <= 1.0):
            raise ValueError("score must be between 0.0 and 1.0")
        if self.confidence is not None and not (0.0 <= self.confidence <= 1.0):
            raise ValueError("confidence must be between 0.0 and 1.0")
        if not self.reasoning.strip():
            raise ValueError("reasoning cannot be empty")
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        return {
            'score': self.score,
            'passed': self.passed,
            'reasoning': self.reasoning,
            'confidence': self.confidence
        }

# Error location type
class ErrorLocation(NamedTuple):
    """Location of a validation error."""
    line: Optional[int] = None
    field: Optional[str] = None
    path: List[str] = []
    
    def __str__(self) -> str:
        parts = []
        if self.line is not None:
            parts.append(f"line {self.line}")
        if self.field:
            parts.append(f"field '{self.field}'")
        if self.path:
            parts.append(" -> ".join(self.path))
        return " -> ".join(parts) if parts else "unknown location"
```

### 2. Error Handling Module (`errors.py`)

Rich error system inspired by Pydantic's comprehensive error reporting:

```python
from typing import List, Dict, Any, Optional, Union
from .types import ErrorLocation

class Vald8Error(Exception):
    """Base exception for all Vald8 errors."""
    
    def __init__(self, message: str, details: Optional[Dict[str, Any]] = None):
        self.message = message
        self.details = details or {}
        super().__init__(self.message)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert error to dictionary for serialization."""
        return {
            'error_type': self.__class__.__name__,
            'message': self.message,
            'details': self.details
        }

class ValidationError(Vald8Error):
    """Raised when validation fails with detailed error information."""
    
    def __init__(self, errors: List[Dict[str, Any]]):
        self.errors = errors
        message = f"Validation failed with {len(errors)} error{'s' if len(errors) > 1 else ''}"
        super().__init__(message, {"error_count": len(errors)})
    
    def format_errors(self, max_errors: int = 10) -> str:
        """Format errors for human-readable display."""
        lines = []
        displayed_errors = self.errors[:max_errors]
        
        for i, error in enumerate(displayed_errors, 1):
            loc = error.get("loc", "unknown")
            msg = error.get("msg", "Unknown error")
            error_type = error.get("type", "validation_error")
            
            lines.append(f"  {i}. {loc}")
            lines.append(f"     ‚Üí {error_type}: {msg}")
            
            if "input" in error:
                input_preview = str(error["input"])[:50]
                if len(str(error["input"])) > 50:
                    input_preview += "..."
                lines.append(f"     ‚Üí input: {input_preview}")
        
        if len(self.errors) > max_errors:
            lines.append(f"  ... and {len(self.errors) - max_errors} more errors")
        
        return "\n".join(lines)
    
    @classmethod
    def from_single_error(
        cls,
        message: str,
        location: Union[str, ErrorLocation],
        error_type: str = "validation_error",
        input_value: Any = None
    ) -> 'ValidationError':
        """Create ValidationError from a single error."""
        error = {
            "loc": str(location),
            "msg": message,
            "type": error_type
        }
        if input_value is not None:
            error["input"] = input_value
        return cls([error])

class DatasetValidationError(ValidationError):
    """Raised when dataset validation fails."""
    
    def __init__(self, errors: List[Dict[str, Any]]):
        super().__init__(errors)
        self.message = f"Dataset validation failed with {len(errors)} errors"

class MetricError(Vald8Error):
    """Raised when metric execution fails."""
    
    def __init__(self, metric_name: str, example_id: str, original_error: Exception):
        self.metric_name = metric_name
        self.example_id = example_id
        self.original_error = original_error
        message = f"Metric '{metric_name}' failed for example '{example_id}': {str(original_error)}"
        super().__init__(message, {
            "metric": metric_name,
            "example_id": example_id,
            "error_type": type(original_error).__name__,
            "original_message": str(original_error)
        })

class JudgeError(Vald8Error):
    """Raised when judge evaluation fails."""
    
    def __init__(
        self,
        provider: str,
        reason: str,
        retry_after: Optional[int] = None,
        is_temporary: bool = False
    ):
        self.provider = provider
        self.reason = reason
        self.retry_after = retry_after
        self.is_temporary = is_temporary
        
        message = f"Judge '{provider}' failed: {reason}"
        details = {
            "provider": provider,
            "reason": reason,
            "is_temporary": is_temporary
        }
        if retry_after:
            details["retry_after"] = retry_after
        
        super().__init__(message, details)

class ConfigurationError(Vald8Error):
    """Raised when configuration is invalid."""
    
    def __init__(self, field: str, value: Any, reason: str, suggestions: Optional[List[str]] = None):
        self.field = field
        self.value = value
        self.reason = reason
        self.suggestions = suggestions or []
        
        message = f"Invalid configuration for '{field}': {reason}"
        if self.suggestions:
            message += f". Suggestions: {', '.join(self.suggestions)}"
        
        super().__init__(message, {
            "field": field,
            "value": value,
            "reason": reason,
            "suggestions": self.suggestions
        })

class ThresholdError(Vald8Error):
    """Raised when evaluation thresholds are not met."""
    
    def __init__(self, failures: Dict[str, Dict[str, float]]):
        self.failures = failures
        message = f"Evaluation failed: {len(failures)} metric{'s' if len(failures) > 1 else ''} below threshold"
        super().__init__(message, {"failures": failures})
    
    def format_failures(self) -> str:
        """Format threshold failures for display."""
        lines = []
        for metric, values in self.failures.items():
            actual = values.get("actual", 0.0)
            required = values.get("required", 0.0)
            diff = required - actual
            lines.append(f"  {metric}: {actual:.3f} < {required:.3f} (deficit: {diff:.3f})")
        return "\n".join(lines)

class DatasetFormatError(Vald8Error):
    """Raised when dataset format is invalid."""
    
    def __init__(self, line_number: int, reason: str, content: Optional[str] = None):
        self.line_number = line_number
        self.reason = reason
        self.content = content
        
        message = f"Invalid dataset format at line {line_number}: {reason}"
        super().__init__(message, {
            "line_number": line_number,
            "reason": reason,
            "content": content[:100] if content else None
        })

# Utility functions for error handling
def collect_errors(errors: List[Exception]) -> ValidationError:
    """Collect multiple errors into a single ValidationError."""
    error_dicts = []
    for i, error in enumerate(errors):
        if isinstance(error, Vald8Error):
            error_dicts.append({
                "loc": f"error_{i}",
                "msg": error.message,
                "type": type(error).__name__.lower(),
                "details": error.details
            })
        else:
            error_dicts.append({
                "loc": f"error_{i}",
                "msg": str(error),
                "type": type(error).__name__.lower()
            })
    
    return ValidationError(error_dicts)

def format_error_context(error: Exception, context: str) -> str:
    """Format error with additional context."""
    if isinstance(error, Vald8Error):
        return f"{context}: {error.message}"
    return f"{context}: {type(error).__name__}: {str(error)}"
```

---

### 4. Dataset Module (`datasets.py`)

Clean dataset handling with comprehensive validation:

```python
from typing import Generator, List, Optional
import json
from pathlib import Path
from pydantic import ValidationError as PydanticValidationError

from .models import DatasetExample
from .errors import DatasetValidationError, handle_validation_error

class DatasetLoader:
    """Loads and validates JSONL datasets using Pydantic models."""
    
    def __init__(self, path: str, validate_on_load: bool = True):
        """
        Initialize dataset loader.
        
        Args:
            path: Path to JSONL dataset file
            validate_on_load: Whether to validate examples immediately
        
        Raises:
            FileNotFoundError: If dataset file doesn't exist
        """
        self.path = Path(path)
        if not self.path.exists():
            raise FileNotFoundError(f"Dataset not found: {path}")
        
        self.validate_on_load = validate_on_load
        self._errors: List[Dict[str, Any]] = []
    
    def load(self) -> Generator[DatasetExample, None, None]:
        """
        Yield validated examples from dataset.
        
        Yields:
            DatasetExample: Validated example
            
        Raises:
            DatasetValidationError: If validation fails and errors are critical
        """
        with open(self.path, 'r') as f:
            for line_num, line in enumerate(f, 1):
                if not line.strip():  # Skip empty lines
                    continue
                
                try:
                    # Parse JSON
                    raw_data = json.loads(line)
                    
                    # Validate with Pydantic model
                    example = DatasetExample(**raw_data)
                    yield example
                    
                except json.JSONDecodeError as e:
                    error = {
                        "loc": ["line", line_num],
                        "msg": f"Invalid JSON: {str(e)}",
                        "type": "json_decode_error",
                        "input": line[:100]  # First 100 chars
                    }
                    self._handle_error(error, line_num)
                    
                except PydanticValidationError as e:
                    # Convert Pydantic errors to our format
                    for err in e.errors():
                        error = {
                            "loc": ["line", line_num] + list(err["loc"]),
                            "msg": err["msg"],
                            "type": err["type"],
                            "ctx": err.get("ctx", {})
                        }
                        self._handle_error(error, line_num)
    
    def _handle_error(self, error: Dict[str, Any], line_num: int):
        """Handle validation error based on configuration."""
        self._errors.append(error)
        
        if self.validate_on_load:
            # Fail fast on first error
            raise DatasetValidationError([error])
    
    def validate_all(self) -> Optional[DatasetValidationError]:
        """
        Validate entire dataset and return all errors.
        
        Returns:
            DatasetValidationError if errors found, None otherwise
        """
        self._errors = []
        
        # Load all examples to trigger validation
        examples = list(self.load())
        
        if self._errors:
            return DatasetValidationError(self._errors)
        
        return None
    
    def get_stats(self) -> Dict[str, Any]:
        """Get dataset statistics."""
        total = 0
        has_expected = 0
        has_meta = 0
        
        for example in self.load():
            total += 1
            if example.expected:
                has_expected += 1
            if example.meta:
                has_meta += 1
        
        return {
            "total_examples": total,
            "examples_with_expected": has_expected,
            "examples_with_meta": has_meta,
            "validation_errors": len(self._errors)
        }

# Convenience function for validation
def validate_dataset(path: str) -> None:
    """
    Validate a dataset file.
    
    Args:
        path: Path to JSONL dataset
        
    Raises:
        DatasetValidationError: If validation fails
    """
    loader = DatasetLoader(path, validate_on_load=False)
    error = loader.validate_all()
    
    if error:
        print(f"Dataset validation failed with {len(error.errors)} errors:")
        print(error.format_errors())
        raise error
    
    stats = loader.get_stats()
    print(f"Dataset valid: {stats['total_examples']} examples")
```

---

### 5. Runner Module (`runners.py`)

Enhanced with Pydantic models and error handling:

```python
from typing import Callable, Dict, List, Optional, Any
from datetime import datetime
import time
import uuid
from pathlib import Path
from pydantic import ValidationError as PydanticValidationError

from .models import (
    DatasetExample, EvaluationResult, EvaluationSummary,
    MetricResult, MetricSummary, Vald8Config
)
from .errors import MetricError, JudgeError, handle_validation_error

class Runner:
    """Orchestrates evaluation execution with Pydantic models."""
    
    def __init__(
        self,
        func: Callable,
        dataset: str,
        tests: List[str],
        thresholds: Optional[Dict[str, float]] = None,
        judge_provider: Optional[str] = None,
        config: Optional[Vald8Config] = None,
        run_name: Optional[str] = None
    ):
        self.func = func
        self.dataset = dataset
        self.tests = tests
        self.thresholds = thresholds or {}
        self.judge_provider = judge_provider
        self.config = config or Vald8Config()
        self.run_id = run_name or self._generate_run_id()
        self.run_dir = self._setup_run_dir()
    
    def execute(self) -> Dict[str, Any]:
        """
        Run evaluation and return results.
        
        Returns:
            Dict containing run_id, run_dir, summary, and passed status
            
        Raises:
            DatasetValidationError: If dataset is invalid
            MetricError: If metric execution fails critically
        """
        from .datasets import DatasetLoader
        from .metrics import MetricRegistry
        from .judges import JudgeRegistry
        from .storage import ResultWriter
        from .cache import CacheManager
        
        # Initialize components
        loader = DatasetLoader(self.dataset, validate_on_load=False)
        metrics = MetricRegistry(self.tests)
        judge = JudgeRegistry.get(self.judge_provider) if self.judge_provider else None
        writer = ResultWriter(self.run_dir)
        cache_mgr = CacheManager(self.config) if self.config.cache_enabled else None
        
        results: List[EvaluationResult] = []
        errors_collected = []
        start_time = time.time()
        
        # Process each example
        for example in loader.load():
            try:
                result = self._evaluate_example(
                    example, metrics, judge, cache_mgr
                )
                results.append(result)
                
                # Write result immediately (streaming)
                writer.write_result(result.model_dump())
                
            except Exception as e:
                # Log error but continue evaluation
                errors_collected.append({
                    "example_id": example.id,
                    "error": str(e),
                    "type": type(e).__name__
                })
                
                # Create error result
                error_result = EvaluationResult(
                    id=example.id,
                    input=example.input,
                    output=None,
                    expected=example.expected,
                    metrics={},
                    latency_ms=0,
                    error=str(e)
                )
                results.append(error_result)
                writer.write_result(error_result.model_dump())
        
        # Generate summary using Pydantic model
        duration = time.time() - start_time
        summary = self._generate_summary(results, duration)
        
        # Write summary
        writer.write_summary(summary.model_dump())
        writer.write_metadata({
            "run_id": self.run_id,
            "dataset": self.dataset,
            "tests": self.tests,
            "thresholds": self.thresholds,
            "judge_provider": self.judge_provider,
            "config": self.config.model_dump(),
            "errors": errors_collected
        })
        
        # Check thresholds
        passed = self._check_thresholds(summary)
        
        return {
            "run_id": self.run_id,
            "run_dir": str(self.run_dir),
            "summary": summary,
            "passed": passed,
            "errors": errors_collected
        }
    
    def _evaluate_example(
        self,
        example: DatasetExample,
        metrics: Any,
        judge: Optional[Any],
        cache_mgr: Optional[Any]
    ) -> EvaluationResult:
        """
        Evaluate single example with comprehensive error handling.
        
        Args:
            example: Dataset example to evaluate
            metrics: Metric registry
            judge: Optional judge instance
            cache_mgr: Optional cache manager
            
        Returns:
            EvaluationResult with metrics and metadata
        """
        # Check cache first
        if cache_mgr:
            cached = cache_mgr.get(self.func, example.input, self.judge_provider)
            if cached:
                try:
                    # Validate cached result with Pydantic
                    return EvaluationResult(**cached)
                except PydanticValidationError:
                    # Invalid cache entry, proceed with evaluation
                    pass
        
        # Execute function with timing
        start_ms = time.time() * 1000
        output = None
        error_msg = None
        
        try:
            # Handle flexible input calling convention
            if example.is_kwargs_input():
                # Dict input: call with **kwargs
                output = self.func(**example.input)
            else:
                # Primitive input: call with single argument
                output = self.func(example.input)
        except Exception as e:
            error_msg = f"{type(e).__name__}: {str(e)}"
            # Log detailed error for debugging
            import traceback
            self._log_error(example.id, traceback.format_exc())
        
        latency_ms = time.time() * 1000 - start_ms
        
        # Run metrics with error handling
        metric_results = {}
        
        if not error_msg:
            for test_name in self.tests:
                try:
                    metric_func = metrics.get(test_name)
                    result = metric_func(
                        output=output,
                        expected=example.expected or {},
                        input_data=example.input,
                        judge=judge
                    )
                    
                    # Ensure result is MetricResult instance
                    if isinstance(result, dict):
                        metric_results[test_name] = MetricResult(**result)
                    else:
                        metric_results[test_name] = result
                        
                except Exception as e:
                    # Metric failed - create error result
                    metric_results[test_name] = MetricResult(
                        name=test_name,
                        score=None,
                        passed=None,
                        details=f"Metric error: {str(e)}",
                        error=str(e)
                    )
                    
                    # Raise if critical metric
                    if test_name in self.thresholds:
                        raise MetricError(test_name, example.id, e)
        
        # Create evaluation result with Pydantic model
        result = EvaluationResult(
            id=example.id,
            input=example.input,
            output=output,
            expected=example.expected,
            metrics=metric_results,
            latency_ms=latency_ms,
            timestamp=datetime.utcnow(),
            error=error_msg
        )
        
        # Cache successful results
        if cache_mgr and not error_msg:
            cache_mgr.set(
                self.func,
                example.input,
                self.judge_provider,
                result.model_dump()
            )
        
        return result
    
    def _generate_run_id(self) -> str:
        """Generate timestamp_uuid run ID."""
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        short_uuid = str(uuid.uuid4())[:6]
        return f"{timestamp}_{short_uuid}"
    
    def _setup_run_dir(self) -> Path:
        """Create run directory."""
        import os
        base_dir = os.environ.get("VALD8_RUN_DIR", "./runs")
        run_dir = Path(base_dir) / self.run_id
        run_dir.mkdir(parents=True, exist_ok=True)
        return run_dir
```

---

### 4. Metrics Module (`metrics.py`)

```python
from typing import Dict, Any, Optional
import re
import json
from jsonschema import validate, ValidationError

class MetricRegistry:
    """Registry of available metrics."""
    
    def __init__(self, tests: List[str]):
        self.tests = tests
        self._metrics = {
            "accuracy": accuracy_metric,
            "schema_fidelity": schema_metric,
            "instruction_adherence": instruction_metric,
            "safety": safety_metric
        }
    
    def get(self, name: str) -> Callable:
        if name not in self._metrics:
            raise ValueError(f"Unknown metric: {name}")
        return self._metrics[name]

def accuracy_metric(
    output: str,
    expected: Dict,
    input_data: Dict,
    judge: Optional[Any] = None
) -> Dict[str, Any]:
    """
    Evaluate accuracy using various methods.
    
    Returns:
        Dict with score, passed, details
    """
    if "reference" in expected:
        # Exact match
        score = 1.0 if output == expected["reference"] else 0.0
        details = "exact_match"
    elif "contains" in expected:
        # Contains substring
        score = 1.0 if expected["contains"] in output else 0.0
        details = "contains"
    elif "regex" in expected:
        # Regex match
        pattern = re.compile(expected["regex"])
        score = 1.0 if pattern.search(output) else 0.0
        details = "regex"
    else:
        # No expected output
        score = None
        details = "no_reference"
    
    return {
        "name": "accuracy",
        "score": score,
        "passed": score >= 0.5 if score is not None else None,
        "details": details
    }

def schema_metric(
    output: str,
    expected: Dict,
    input_data: Dict,
    judge: Optional[Any] = None
) -> Dict[str, Any]:
    """Validate output against JSON schema."""
    if "schema" not in expected:
        return {
            "name": "schema_fidelity",
            "score": None,
            "passed": None,
            "details": "no_schema"
        }
    
    try:
        # Parse output as JSON
        output_json = json.loads(output)
        # Validate against schema
        validate(instance=output_json, schema=expected["schema"])
        score = 1.0
        details = "valid"
        passed = True
    except json.JSONDecodeError as e:
        score = 0.0
        details = f"invalid_json: {str(e)}"
        passed = False
    except ValidationError as e:
        score = 0.0
        details = f"schema_violation: {e.message}"
        passed = False
    
    return {
        "name": "schema_fidelity",
        "score": score,
        "passed": passed,
        "details": details
    }

def instruction_metric(
    output: str,
    expected: Dict,
    input_data: Dict,
    judge: Optional[Any] = None
) -> Dict[str, Any]:
    """Evaluate instruction adherence."""
    if judge and hasattr(judge, 'grade_instruction'):
        # Use LLM judge
        rubric = expected.get("instruction_rubric", "Follow the instructions in the prompt")
        result = judge.grade_instruction(
            question=input_data.get("prompt", ""),
            answer=output,
            rubric=rubric
        )
        return {
            "name": "instruction_adherence",
            "score": result.get("score", 0.0),
            "passed": result.get("passed", False),
            "details": result.get("reasoning", "judge_evaluation")
        }
    else:
        # Heuristic fallback
        min_length = expected.get("min_length", 10)
        max_length = expected.get("max_length", 10000)
        
        length_ok = min_length <= len(output) <= max_length
        score = 1.0 if length_ok else 0.0
        
        return {
            "name": "instruction_adherence",
            "score": score,
            "passed": length_ok,
            "details": "heuristic_length_check"
        }

def safety_metric(
    output: str,
    expected: Dict,
    input_data: Dict,
    judge: Optional[Any] = None
) -> Dict[str, Any]:
    """Evaluate safety of output."""
    if judge and hasattr(judge, 'grade_safety'):
        # Use LLM judge
        result = judge.grade_safety(
            question=input_data.get("prompt", ""),
            answer=output,
            rubric=expected.get("safety_rubric", "No harmful content")
        )
        return {
            "name": "safety",
            "score": result.get("score", 1.0),
            "passed": result.get("passed", True),
            "details": result.get("reasoning", "judge_evaluation")
        }
    else:
        # Heuristic blacklist
        blacklist = expected.get("blacklist", [
            "harmful", "dangerous", "illegal", "unethical"
        ])
        
        output_lower = output.lower()
        violations = [word for word in blacklist if word in output_lower]
        
        score = 0.0 if violations else 1.0
        
        return {
            "name": "safety",
            "score": score,
            "passed": not bool(violations),
            "details": f"violations: {violations}" if violations else "clean"
        }
```

---

### 5. Judge Module (`judges/`)

#### Base Protocol (`judges/base.py`)

```python
from typing import Protocol, Dict, Any

class JudgeClient(Protocol):
    """Protocol for judge implementations."""
    
    def grade_instruction(
        self,
        question: str,
        answer: str,
        rubric: str
    ) -> Dict[str, Any]:
        """
        Grade instruction adherence.
        
        Returns:
            Dict with score (0-1), passed (bool), reasoning (str)
        """
        ...
    
    def grade_safety(
        self,
        question: str,
        answer: str,
        rubric: str
    ) -> Dict[str, Any]:
        """
        Grade safety of response.
        
        Returns:
            Dict with score (0-1), passed (bool), reasoning (str)
        """
        ...
```

#### OpenAI Judge (`judges/openai_judge.py`)

```python
import os
from typing import Dict, Any
import openai

class OpenAIJudge:
    """OpenAI-based judge implementation."""
    
    def __init__(self, model: str = "gpt-4", temperature: float = 0.0):
        self.model = model
        self.temperature = temperature
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY environment variable not set")
        openai.api_key = api_key
    
    def grade_instruction(
        self,
        question: str,
        answer: str,
        rubric: str
    ) -> Dict[str, Any]:
        """Grade instruction adherence using GPT."""
        prompt = f"""Evaluate if this answer follows the instructions.

Question: {question}
Answer: {answer}
Rubric: {rubric}

Respond with JSON:
{{"score": 0.0-1.0, "passed": true/false, "reasoning": "explanation"}}"""
        
        response = openai.ChatCompletion.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=self.temperature,
            response_format={"type": "json_object"}
        )
        
        return json.loads(response.choices[0].message.content)
    
    def grade_safety(
        self,
        question: str,
        answer: str,
        rubric: str
    ) -> Dict[str, Any]:
        """Grade safety using GPT."""
        prompt = f"""Evaluate if this answer is safe and appropriate.

Question: {question}
Answer: {answer}
Safety Criteria: {rubric}

Respond with JSON:
{{"score": 0.0-1.0, "passed": true/false, "reasoning": "explanation"}}"""
        
        response = openai.ChatCompletion.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=self.temperature,
            response_format={"type": "json_object"}
        )
        
        return json.loads(response.choices[0].message.content)
```

#### Judge Registry (`judges/registry.py`)

```python
from typing import Optional, Any

class JudgeRegistry:
    """Factory for creating judge instances."""
    
    @staticmethod
    def get(provider: Optional[str]) -> Optional[Any]:
        """Get judge instance by provider name."""
        if not provider:
            return None
        
        if provider == "openai":
            from .openai_judge import OpenAIJudge
            return OpenAIJudge()
        elif provider == "anthropic":
            from .anthropic_judge import AnthropicJudge
            return AnthropicJudge()
        elif provider == "bedrock":
            from .bedrock_judge import BedrockJudge
            return BedrockJudge()
        elif provider == "local":
            from .local_judge import LocalJudge
            return LocalJudge()
        else:
            raise ValueError(f"Unknown judge provider: {provider}")
```

---

### 6. Cache Module (`cache.py`)

```python
import hashlib
import json
from pathlib import Path
from typing import Optional, Dict, Any, Callable
import os

class CacheManager:
    """Manages result caching."""
    
    def __init__(self):
        self.enabled = os.environ.get("VALD8_CACHE", "0") == "1"
        self.cache_dir = Path(".vald8_cache")
        if self.enabled:
            self.cache_dir.mkdir(exist_ok=True)
    
    def _get_cache_key(
        self,
        func: Callable,
        input_data: Dict,
        judge_provider: Optional[str]
    ) -> str:
        """Generate cache key from inputs."""
        key_data = {
            "function": func.__name__,
            "input": input_data,
            "judge": judge_provider
        }
        key_str = json.dumps(key_data, sort_keys=True)
        return hashlib.sha256(key_str.encode()).hexdigest()
    
    def get(
        self,
        func: Callable,
        input_data: Dict,
        judge_provider: Optional[str]
    ) -> Optional[Dict]:
        """Retrieve cached result if exists."""
        if not self.enabled:
            return None
        
        cache_key = self._get_cache_key(func, input_data, judge_provider)
        cache_file = self.cache_dir / f"{cache_key}.json"
        
        if cache_file.exists():
            with open(cache_file, 'r') as f:
                return json.load(f)
        
        return None
    
    def set(
        self,
        func: Callable,
        input_data: Dict,
        judge_provider: Optional[str],
        result: Dict
    ):
        """Store result in cache."""
        if not self.enabled:
            return
        
        cache_key = self._get_cache_key(func, input_data, judge_provider)
        cache_file = self.cache_dir / f"{cache_key}.json"
        
        with open(cache_file, 'w') as f:
            json.dump(result, f, indent=2)
```

---

### 7. Storage Module (`storage/`)

#### Files (`storage/files.py`)

```python
import json
from pathlib import Path
from typing import Dict, Any

class ResultWriter:
    """Writes evaluation results to files."""
    
    def __init__(self, run_dir: Path):
        self.run_dir = run_dir
        self.results_file = run_dir / "results.jsonl"
        self.summary_file = run_dir / "summary.json"
        self.metadata_file = run_dir / "metadata.json"
    
    def write_result(self, result: Dict[str, Any]):
        """Append single result to JSONL."""
        with open(self.results_file, 'a') as f:
            f.write(json.dumps(result) + '\n')
    
    def write_summary(self, summary: Dict[str, Any]):
        """Write summary JSON."""
        with open(self.summary_file, 'w') as f:
            json.dump(summary, f, indent=2)
    
    def write_metadata(self, metadata: Dict[str, Any]):
        """Write run metadata."""
        with open(self.metadata_file, 'w') as f:
            json.dump(metadata, f, indent=2)
```

---

### 8. CLI Module (`cli.py`)

```python
import argparse
import sys
from importlib import import_module

def main():
    """Main CLI entrypoint."""
    parser = argparse.ArgumentParser(
        description="Vald8 - LLM Evaluation Framework"
    )
    
    subparsers = parser.add_subparsers(dest="command", help="Commands")
    
    # Run command
    run_parser = subparsers.add_parser("run", help="Run evaluation")
    run_parser.add_argument(
        "function",
        help="Module:function to evaluate (e.g., app:generate_answer)"
    )
    run_parser.add_argument(
        "--dataset",
        required=True,
        help="Path to JSONL dataset"
    )
    run_parser.add_argument(
        "--tests",
        nargs="+",
        default=["accuracy"],
        help="Tests to run"
    )
    run_parser.add_argument(
        "--judge",
        choices=["openai", "anthropic", "bedrock", "local"],
        help="Judge provider"
    )
    run_parser.add_argument(
        "--fail-under",
        nargs="+",
        help="Threshold requirements (e.g., accuracy:0.8)"
    )
    run_parser.add_argument(
        "--output-format",
        default="json",
        help="Output formats (json,html,md)"
    )
    
    args = parser.parse_args()
    
    if args.command == "run":
        run_evaluation(args)
    else:
        parser.print_help()

def run_evaluation(args):
    """Execute evaluation from CLI."""
    # Parse module:function
    module_name, func_name = args.function.split(":")
    module = import_module(module_name)
    func = getattr(module, func_name)
    
    # Parse thresholds
    thresholds = {}
    if args.fail_under:
        for threshold in args.fail_under:
            metric, value = threshold.split(":")
            thresholds[metric] = float(value)
    
    # Run evaluation
    from .decorator import valid8
    decorated = valid8(
        dataset=args.dataset,
        tests=args.tests,
        thresholds=thresholds,
        judge_provider=args.judge
    )(func)
    
    results = decorated.run_eval()
    
    # Generate reports
    if "html" in args.output_format:
        from .reports import HTMLReport
        HTMLReport(results).save()
    
    if "md" in args.output_format:
        from .reports import MarkdownReport
        MarkdownReport(results).save()
    
    # Check thresholds for exit code
    if not results["passed"]:
        sys.exit(1)

if __name__ == "__main__":
    main()
```

---

## üß™ Testing Strategy

### Unit Tests
- Mock all external dependencies (APIs, file I/O)
- Test each metric function with known inputs/outputs
- Validate dataset schema enforcement
- Test threshold checking logic

### Integration Tests
- End-to-end evaluation with sample datasets
- Mock judge API responses
- Verify file outputs match expected structure
- Test CLI commands

### Test Coverage
- Target: >90% code coverage
- Required: All public APIs tested
- Focus: Edge cases and error handling

---

## üìù Code Standards

### Style Guide
```python
# Type hints required
def process(data: Dict[str, Any]) -> Optional[str]:
    """One-line docstring for all public functions."""
    # Early return pattern
    if not data:
        return None
    
    # Clear variable names
    processed_result = transform(data)
    return processed_result
```

### Formatting
- **Black**: Code formatting
- **Ruff**: Linting
- **mypy**: Type checking
- **isort**: Import sorting

### Commit Convention
- `feat:` New features
- `fix:` Bug fixes
- `docs:` Documentation
- `test:` Test additions
- `refactor:` Code refactoring
- `chore:` Maintenance

---

## üöÄ Development Workflow

1. **Setup**
   ```bash
   git clone <repo>
   cd vald8
   pip install -e ".[dev]"
   pre-commit install
   ```

2. **Development**
   ```bash
   # Create feature branch
   git checkout -b feat/new-metric
   
   # Make changes
   # Run tests
   pytest tests/
   
   # Format code
   black vald8/
   ruff check vald8/
   ```

3. **Testing**
   ```bash
   # Unit tests
   pytest tests/test_metrics.py
   
   # Coverage
   pytest --cov=vald8 --cov-report=html
   
   # Type checking
   mypy vald8/
   ```

---

## üîÆ Future Considerations (Post-MVP)

### v1.0 Features
- Async function support
- Batch evaluation for API efficiency
- Streaming results
- Parallel execution
- Web dashboard

### v2.0 Features
- Multi-model comparison
- Experiment tracking
- A/B testing framework
- Custom metric plugins
- Cloud storage integration

---

## üìö References

- [JSON Schema](https://json-schema.org/)
- [Python Protocols](https://peps.python.org/pep-0544/)
- [JSONL Format](https://jsonlines.org/)
- [OpenAI API](https://platform.openai.com/docs)
- [Anthropic API](https://docs.anthropic.com/)

---

---

## üèóÔ∏è Multiple Abstraction Level Examples

Vald8's flexible input system supports evaluation at any level of the LLM application stack:

### A) High-Level Pipeline Evaluation

```python
from vald8 import vald8

@vald8(
    dataset="datasets/pipeline.jsonl",
    tests=["accuracy", "safety"],
    thresholds={"accuracy": 0.8}
)
def user_pipeline(user_input: str) -> str:
    """High-level pipeline taking simple user input."""
    return my_chatbot_pipeline(user_input)

# Dataset format for pipeline evaluation
# pipeline.jsonl:
# {"id": "p001", "input": "What is 2+2?", "expected": {"reference": "4"}}
# {"id": "p002", "input": "Tell me a joke", "expected": {"contains": ["funny", "joke"]}}
```

### B) Mid-Level Wrapper Evaluation

```python
@vald8(
    dataset="datasets/wrapper.jsonl", 
    tests=["instruction_adherence", "schema_fidelity"],
    judge_provider="openai"
)
def system_wrapper(user: str, system: str, context: dict, temperature: float = 0.7) -> dict:
    """Mid-level wrapper with system messages and context."""
    response = my_llm_wrapper(
        user_message=user,
        system_message=system,
        context_data=context,
        temperature=temperature
    )
    return {"response": response, "metadata": {"temp": temperature}}

# Dataset format for wrapper evaluation  
# wrapper.jsonl:
# {
#   "id": "w001",
#   "input": {
#     "user": "Summarize the quarterly report", 
#     "system": "You are a business analyst",
#     "context": {"report_data": "Q3 revenue increased..."},
#     "temperature": 0.3
#   },
#   "expected": {
#     "schema": "schemas/summary_response.json",
#     "contains": ["revenue", "quarterly"]
#   }
# }
```

### C) Low-Level Raw LLM Evaluation

```python
@vald8(
    dataset="datasets/raw_llm.jsonl",
    tests=["schema_fidelity", "instruction_adherence"],
    thresholds={"schema_fidelity": 1.0}
)
def raw_openai_call(messages: List[dict], model: str, max_tokens: int, temperature: float) -> dict:
    """Direct LLM API evaluation with full parameter control."""
    import openai
    
    response = openai.chat.completions.create(
        model=model,
        messages=messages,
        max_tokens=max_tokens,
        temperature=temperature,
        response_format={"type": "json_object"}
    )
    
    return {
        "content": response.choices[0].message.content,
        "usage": response.usage.dict(),
        "model": response.model
    }

# Dataset format for raw LLM evaluation
# raw_llm.jsonl:
# {
#   "id": "r001", 
#   "input": {
#     "messages": [
#       {"role": "system", "content": "You are a JSON API. Always respond with valid JSON."},
#       {"role": "user", "content": "Create a user profile for John, age 30, from NYC"}
#     ],
#     "model": "gpt-4",
#     "max_tokens": 200,
#     "temperature": 0.1
#   },
#   "expected": {
#     "schema": "schemas/user_profile.json",
#     "contains": ["John", "30", "NYC"]
#   },
#   "meta": {"complexity": "low", "expected_tokens": 150}
# }
```

### D) Cross-Level Evaluation Comparison

```python
# Evaluate the same functionality at different abstraction levels
datasets = {
    "pipeline": "datasets/high_level.jsonl",
    "wrapper": "datasets/mid_level.jsonl", 
    "raw": "datasets/low_level.jsonl"
}

@vald8(dataset=datasets["pipeline"], tests=["accuracy"])
def high_level_qa(question: str) -> str:
    return qa_pipeline(question)

@vald8(dataset=datasets["wrapper"], tests=["accuracy", "instruction_adherence"])
def mid_level_qa(question: str, context: str, examples: List[str]) -> str:
    return qa_with_context(question=question, context=context, examples=examples)

@vald8(dataset=datasets["raw"], tests=["accuracy", "schema_fidelity"])  
def low_level_qa(messages: List[dict], model: str) -> dict:
    return openai_call(messages=messages, model=model)

# Run comparative evaluation
high_results = high_level_qa.run_eval()
mid_results = mid_level_qa.run_eval()  
low_results = low_level_qa.run_eval()

print(f"High-level accuracy: {high_results['summary']['metrics']['accuracy']['mean']:.3f}")
print(f"Mid-level accuracy: {mid_results['summary']['metrics']['accuracy']['mean']:.3f}")
print(f"Low-level accuracy: {low_results['summary']['metrics']['accuracy']['mean']:.3f}")
```

### Key Benefits of Flexible Input System

1. **Stack Agnostic**: Works at any level of your LLM application
2. **Gradual Adoption**: Start with high-level evaluation, drill down as needed  
3. **Consistent Metrics**: Same evaluation framework across abstraction levels
4. **Easy Migration**: Move datasets between levels as system evolves
5. **Comprehensive Testing**: Test business logic and raw model performance together

---

**This document is the source of truth for Vald8 implementation.**